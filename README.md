# Function-Transformation-Tasks
This repository contains code for Function Transformation Tasks created by Martin Finn. In particular, the Car Race task described by Finn and De Houwer (2021) and the Bubble-clicking task described in Finn et al. (2023).

## File Description

The repository contains a folder for each task. Each folder includes the manuscript in which the task was first described, along with all files (preregistration, scripts, analyses) to run the studies in the manuscript. The folder also contains a script where the task is isolated.

*Folder Car Race Task:*
- Draft manuscript
- Folder *data + R scripts* contains raw data and R-scripts for processing and analyses
- Folder *preregistrations* contains reregistration documents for four experiments
- Folder *materials* contains json files to run four experiments. These be downloaded and subsequently uploaded and run on https://labjs.felixhenninger.com/

*Folder Bubbles Task:*
- Preregistration documents for four experiments + publisher accepted version of Finn & De Houwer (2021) JEAB manuscript.
- Folder *data + R scripts* contains raw data and R-scripts for processing and analyses
- Folder *labjs scripts* contains json files to run four experiments. These be downloaded and subsequently uploaded and run on https://labjs.felixhenninger.com/

## Task description

### Car Race Task

The procedure was designed to establish Crel and Cfunc functions for stimuli and assess their efficacy in specifying derived transformations of stimulus functions. The procedure can be decomposed into five phases: phase 1) establishing Crels for the relations of same and different; phase 2) testing Crels for the relations of same and different; phase 3) establishing selective action of Cfuncs for speed and direction on transformations of functions via relations of same and different; phase 4) testing the selective action of Crels and Cfuncs established in the previous phase; phase 5) testing for the generalization of the experimentally established Crels and Cfuncs. All measures were programmed in java script using lab.js (Henninger et al., 2019).


**Phases 1 and 2: establishing and testing Crels.** <br>
In phase 1 a matching to sample (MTS) procedure was employed to establish Crels for the relations of same and different. The MTS began with the instruction “In this task you will see a sequence of screens. On these screens you must respond by selecting either E or I on your keyboard. Learn how to respond accurately. Then respond as accurately as you can.” Before each block of trials, they were instructed “Respond as accurately as you can. If your accuracy is less than 85% this phase will begin again”. Each trial began with a 300ms stimulus onset interval (SOI). The sample stimulus, the word *“SAME”* or *“DIFFERENT”*, appeared at the top center of the screen, and the comparison stimuli, the symbols ◼ and ▼, were presented at the bottom left and bottom right of the screen. Arbitrary shapes were established as Crels because this is standard practice in research on derived stimulus relations, and thus the experimental analysis reported here might complement and extend existing RFT-based experimental analyses of derived stimulus relations. The locations of the stimuli were counterbalanced across trials. Selections for the left and right comparisons were made with the ‘E’ and ‘I’ keys. Immediately after correct selections the message “Correct!” appeared in green text at the center of the screen for 500ms. The message *“Wrong!”* appeared in red text in the same location for 1000ms after incorrect responses. The MTS presented up to five 30 trial blocks, and terminated early if participants achieved an accuracy score ≥ 17/20 across the previous 20 trials in a block.

Phase 2 began after the MTS. The Crel properties of the symbols ◼ and ▼ were tested in a delayed MTS used in a previous study (https://osf.io/w2n9g/). First, a Crel was presented for 1000 ms. After an SOI of 300 ms, participants were presented with an arrangement of three circles of equal size, two of which were the same color, that served as sample and comparison stimuli (e.g., a red circle at the top of the screen, a red circle at the bottom left, and a green circle at the bottom right). Participants received the instruction *“Select the appropriately colored circle”*. They then selected a comparison stimulus after the presentation of each Crel (e.g., selecting the circle that is the same color as the circle at the top of the screen after seeing the Crel for same). No response feedback was provided during this 30-trial test block.

**Phases 3, 4 and 5: establishing and testing Cfuncs.** <br>
Phases 3 and 4 employed the car race paradigm. Each trial of the paradigm consisted of two parts, a sample racecar screen which is followed by the race screen. The layout of these components is shown in Figure 1. The sample racecar screen illustrated in the left panel of Figure 1 presented a sample racecar in the center of the screen, a finish line to the left or the right of the screen, and selection boxes at the bottom of the screen. Each selection box displayed the sample racecar on the left, a racecar on the right, as well as two pairs of Crels (i.e., ◼ or ▼) and Cfuncs (i.e.,  or ) between the racecars. The selection boxes specified via the Crels and Cfuncs how each racecar would perform compared to the sample racecar. Within each selection box the Crels were always located to the left of the Cfuncs, but the Cfuncs for speed and direction were presented in the upper and lower positions within each selection box an approximately even number of times across trials. Five different racecar stimuli were presented across the task as a whole, with a maximum of four particular racecar stimuli being presented in a given trial (i.e., one sample racecar, and three racecars). The five racecar stimuli appeared equally often in each location throughout the study. This sample racecar screen also presented a white start line, and a checkered finish line. The white start line remained in a fixed location at the center of the screen. The finish line was presented at the same location on both screens within a trial, but appeared at the left and right of the screen an approximately equal number of times across trials. This manipulation ensured that the direction of the racecars was relevant to selecting the winning racecar.

**Figure 1** <br>
*An Example of a Trial from the Car Race Paradigm* <br>
<img width="945" height="258" alt="image" src="https://github.com/user-attachments/assets/3a0594f6-12b6-4d6e-ba48-7722d983be49" />


Clicking a button containing the text *“start sample racecar”* on the sample racecar screen showed participants how the sample racecar performed. In each trial the sample racecar moved to the left or right, and did so quickly or slowly. In trials where the sample racecar moved quickly it moved approximately 3 times faster than in trials when it moved slowly. Specifically, slow racecars were programmed to move 2 pixels every 1 ms, whereas fast racecars were programmed to move at a rate of 6 pixels every 1 ms. The performance of the sample racecar in conjunction with the combinations of Crels and Cfuncs could specify four different ways each racecar may perform relative to the sample racecar (i.e., same direction and same speed, same direction and different speed, different direction and same speed, or different direction and different speed). There was a maximum of three racecars that could be selected in each trial, so only three kinds of performances were possible. Table 1 below provides an overview of the ways the sample racecar could perform, and how the performances of possible winning racecars compared to the performance of the sample racecar. In this context it is important to bear in mind that the position of the finish line, the performance of the sample racecar relative to the finish line (i.e., whether it went in the correct direction), and the specified performance of the racecars were relevant to selecting the winning racecar on each trial. When participants made their selection, they progressed to the race screen where the racecar they selected was indicated by a green line (see right panel of Figure 1). During the race screen, the racecars moved across the screen either to the left or to the right and did so either quickly or slowly. After a race was completed, participants were informed whether they selected the appropriate racecar with the message *“Your racecar won!”* presented in green for 500ms or *“Your racecar did not win!”* presented in red for 1000 ms.

Phase 3 began with a brief walkthrough that oriented participants to the elements described above. The walkthrough, which lasted approximately two minutes, used text boxes and various animations to highlight the various features of the task. To proceed though the walkthrough participants used their mouse to select buttons within the textboxes or on other points of the screen. It began with the instruction: *"In the next part of this study you will see racecars complete a number of car races. Before each race you will select the racecar you think will win the next race. To help you choose the winner of the next race you will be provided with some information about how each racecar will perform in the next race. Before you begin we will show you all the important parts of the task. Please read this information carefully."*

The walkthrough presented a screen like that shown in the left panel of Figure 1, along with the text *“Before each race begins you will see a screen like this.”*. After clicking continue an animation periodically altered the transparency of the sample racecar in the center of the screen and in the selection boxes, and the textbox text changed to *“The sample racecar is at the center of the screen. At the bottom of the screen there are three boxes. In each box the sample racecar is presented to the left of three racecars.”*. Clicking continue stopped the animation for the sample racecar, instead animating the racecars on the right of the selection boxes, and changed the text to *“It is important to know that the sample racecar never participates in the next race. Only the racecars to the right of each box participate in the next race.”*

Next, the symbols between the racecars were animated, and the text read *“The symbols in the middle of each box indicate how the performance of each racecar compares to the sample racecar. The symbols presented here are just examples.”* After this the text informed participants: *"The symbols tell you how the performance of each racecar compares to the sample racecar. The only way to predict the winning racecar is by understanding what the symbols tell you about how the performance of each racecar compares to the performance of the sample racecar. For this reason it is important to know how the sample racecar performs."* At this point the text read *“You can see the performance of the sample racecar by pressing the START SAMPLE RACECAR button.”* Clicking this button resulted in the sample racecar moving across the screen. Next participants were told *“The performance of the sample racecar changes from race to race. To help you see the difference we will show you four sample racecars at the same time. In other parts of the study you will see only one sample racecar before a race.”* Continuing produced four sample racecars vertically aligned in the center of the screen, and the instruction *“You can see the different ways the sample racecar might perform by pressing the START SAMPLE RACECAR button.”* Clicking the button showed all four possible movements of the sample racecars (i.e., moving quickly to the left, slowly to the left, quickly to the right, and slowly to the right). When the sample racecars reached the finish line the text read *“Pressing the START SAMPLE RACECAR button again will allow you see the different performances once more. If you do not want to see the performances again, press the continue button below.”*

Pressing the Continue button began the sequence illustrating how the finish line would change locations. The text changed to *“The performance of the sample racecar is not the only thing that will change from race to race”.* This sequence also showed four sample racecars of differing performances. The finish line began to the right of the screen, the text read *“The finish line will sometimes appear on the right. The racecar at the top wins!”* [fast to the right]. The four sample racecars were returned to the center of the screen, the finish line then moved to the left of the screen, and a new race commenced with the instruction *“At other times the finish line will appear on the left. The racecar in the middle wins!”* [fast to the left]. The walkthrough ended with the instruction: *"Now you have seen the important parts of this task: - the sample racecar and the different ways it might perform - the racecars that participate in the next race - the symbols that indicate how the racecars performances compare to the sample racecar - the changing location of the finish line. To continue to the race screen select a racecar by pressing the select button in one of the three boxes at the bottom of the screen."*
After this instruction participants progressed to the racecar screen (right panel of Figure 1), which showed the performance of the three racecars.
Phase 3 properly began upon completion of the walkthrough. In phase 3 participants were exposed to the training designed to establish the selective action of Cfuncs for the properties of speed and direction. The task instructions were: "Your goal is to select the winning racecar as often as you can. The performance of each racecar changes from race to race. The only way to predict the winning racecar is by understanding what the symbols tell you about how the performance of each racecar compares to the performance of the sample racecar. To select the winning racecar, pay attention to the sample racecar and the symbols in the boxes at the bottom of the screen. Use the information provided to select the winning racecar as often as you can. In the beginning this task will involve some trial and error. Please be patient and try your best to select the winning racecar. You will see in the race if you guessed correctly. To make sure you know whether you selected the winner, you will be told after each race if you selected the winning racecar. Good luck! 

Each block began with the instruction *“Select as many winners as you can. If your accuracy is less than 85% this phase will begin again”*. Participants were required to select the racecar they thought would win the next race. Blocks consisted of thirty races and terminated early if participants achieved an accuracy score ≥ 17 across the previous 20 trials within a block.
The Cfunc for direction was established before the Cfunc for speed. The programmed Cfuncs were the symbols  and , and the property each symbol specified (i.e., speed or direction) was counterbalanced across participants. Initially in phase 3 only two selection boxes appeared beneath the sample racecar (see left panel of Figure 2). When establishing the Cfunc for direction, the speed of the two racecars bore the same relation to the sample racecar. Thus, only the Cfunc for direction differed across racecars within trials and across trials within blocks. When establishing the Cfunc for speed, the direction of the two racecars was related to the sample racecar in the same way, and so only the Cfunc for speed varied across racecars and trials. Participants also received additional training trials in which the racecar stimuli varied along both Cfunc dimensions. This involved trials in which three selection boxes appeared beneath the sample racecar (see the left panel of Figure 1) and both the speed and direction of these three racecars varied within and across trials, and by extension so did their relationships to the properties of the sample racecar. Each of these three kinds of training just described (i.e., training for the Cfunc for direction, training for the Cfunc for speed, and mixed Cfunc training) lasted up to 5 blocks, terminating early when a participant met the training criterion. Phase 3 terminated when these three kinds of training blocks had been completed.

**Figure 2**<br>
*Two Illustrative Examples of Sample Racecar Screens Used to Establish Each Cfunc* <br>
<img width="945" height="264" alt="image" src="https://github.com/user-attachments/assets/95efa718-dc38-42b8-b4cc-121ba435eab6" />

The selective action of the experimentally established Crels and Cfuncs was tested in two ways. The first test comprised phase 4 of the experiment, and took the same format as the car race with three selection boxes described above, and simply involved the removal of the programmed consequences for two 30-trial blocks. After completing this, participants were shown the four symbols and asked to type their responses to the question “What do you think each of these symbols mean?”. They were also provided a space to make other comments. Phase 5 of the experiment involved the second test of the established Crels and Cfuncs. This generalization test took a similar format to the final test employed by Stewart et al. (2013) which assessed Crel and Cfunc control over selection responses. The selection-based test assessed whether the established Crels and Cfuncs generalized to relations between new stimuli. The stimuli were two pictures each of a bicycle, a truck, a helicopter and a plane, one in which the vehicle was oriented to the left and one with the vehicle oriented to the right. Before completing the generalization test participants rated each stimulus in terms of their speed via a six-item scale from “Very slow” to “Very fast”, and their direction as either “To the left” or “To the right”. The instruction before the generalization test read: *"In the next phase you will now see pictures of different kinds of vehicles presented together. Similar to previous phases of this task you will see symbols presented between these vehicles. Based on your experiences with these symbols in the study so far, your goal is to select the appropriate box. Respond as accurately as you can."*

On a given trial in the generalization test a pair of stimuli were presented in four selection boxes similar to those described above for Cfunc training. The four selection boxes differed in the precise constellation of Crels and Cfuncs that appeared between the vehicle stimuli (see Figure 3). Participants were required to select the appropriate box for each pair of stimuli and arrangement of Crels and Cfuncs. The generalization test consisted of 30 trials without response feedback.

**Figure 3** <br>
*A Screenshot of a Trial in the Generalization Test in Phase 5* <br>
<img width="607" height="358" alt="image" src="https://github.com/user-attachments/assets/f13d1f29-0960-427d-95f8-e03d189fbb55" />


### Bubble-Clicking Task

**Phase 1: walkthrough and calibration of bubble-clicking tasks** <br>
The bubbles task began with a walkthrough that described the task to participants. Participants were informed they would complete a series of trials involving choices between tasks, and completion of the chosen
tasks. They were shown an example selection screen the various elements of which are illustrated; the source, the miniature source task, the task options and positions they may occupy relative to the source, the
presence of symbols indicating how the task options compare to the source task. The symbols for which Crel and Cfunc functions were to be established were not presented in the walkthrough. The calibration
phase began immediately after the walkthrough. The purpose of calibration was to ensure that changes in the number of bubbles within the task bore upon the likelihood of earning points. During calibration
participants completed a series of seven trials in which the source task was the only available option. The first trial presented seven bubbles to be clicked within the 5 s time window. Participants were provided up to 10 opportunities to complete the first trial. Each subsequent trial increased the number of bubbles to be clicked within this time window by two. The sequence terminated when the participant failed to complete a trial. The number of bubbles clicked in the final successful trial was the calibrated set point for that participant. This calibrated set point could be 7, 9, 11, 13, 15, 17, or 19.

**Phase 2: establishing Crels and Cfuncs** <br>
In this part of the task participants were provided with choices between two tasks that differed along task relevant dimensions (i.e., number of bubbles, number of points; Fig. 1a). On each trial participants first viewed the miniature source task that displayed a number of moving bubbles that varied across trials around the calibrated set point for that participant (i.e., the calibrated set point ± 1; Fig. 1b) and a
message indicating the number of points on offer (i.e., 50 points ± 5 points) that appeared immediately above the miniature source task (see Fig. 1b). The source task could not be selected, but the two other task
options displayed on each selection screen could. The task options were presented at an equal distance from the source task, and can appear at 30˚, 90 ˚, 150 ˚, 210 ˚, 270 ˚, and 330 ˚angles relative to the source task. The precise location of each option, and its angle relative to the source task was counterbalanced across trials. The manner in which the properties of each task differed from the source task was specified by Crels and Cfuncs appearing on the arrows between the source stimulus and the stimuli representing the alternative task options (see Fig. 1c). In this experiment the Crel and Cfunc stimuli were drawn from natural language (i.e., “more points”, “fewer points”, “more bubbles”, and “fewer bubbles”). On each trial there was an optimal choice. To ensure this is the case on every trial participants were offered choices between the options making up the following pairs: more points or less points, more points or more bubbles, less bubbles or less points, and less bubbles or
more bubbles. Note that the first option in each pair was deemed the optimal choice. Upon selecting a task option participants were exposed to a post-choice feedback screen (see Fig. 1d). This screen was presented for between 3 and 10 s and could be terminated by clicking a continue button after 3 s. The screen displayed a miniature version of each of the alternative task options including the number of bubbles and the message indicating the number of points on offer. The task the participant selected on the previous screen appeared within a green border. The purpose of this component was to facilitate discriminating the difference between the task options. Following the post-choice feedback screen participants were exposed to a bubbles task with the specified properties (e.g., Fig. 1e). Specifically, relative to the source, less bubbles meant a 50% decrease in bubbles, more bubbles meant a 50% increase in the number of bubbles, more points increased the number of points by 50, and less points decreased the number of points by 40. Responses in each trial were deemed correct and counted toward the calculation of training criterion when participants selected the optimal choice and successfully completed the selected bubble task. A trial was deemed incorrect if a participant failed to fulfill either of these criteria. This phase comprised five 30 trial blocks, and terminated either upon completion of these five
blocks, or upon reaching the training criterion of 17 or more correct trials across the previous 20 trials within a block.

**Phase 3: testing the established Crels and Cfuncs** <br>
In this part of the task participants completed trials involving task options multiple steps from the source. The format of the selection screen in Phase 3 trials is illustrated in Fig. 1c and thus differs from the selection screen in Phase 2 as illustrated in Fig. 1a. The source was always presented in the center of the screen. The task options one step removed from the source appeared at opposite sides of the source at 90 ˚and 270 ˚, 30 ˚and 210 ˚, and 150 ˚and 330 ˚angles relative to the source respectively. This ensured that all task options appeared equidistant from the source. The exact locations, and natural language stimuli appearing between the task options were counterbalanced across trials. As in Phase 2, there was an optimal choice on each trial. The optimal choice was always two-steps removed from the source and involved
either 25% fewer bubbles, 100 more points, or 50 more points and 50% fewer bubbles. Responses were deemed correct when the optimal choice had been selected and the related task was successfully completed. Unlike Phase 2 the post-choice feedback screen (Fig. 1d) was not presented in Phase 3. Phase 3 involved two 30 trial blocks and terminated upon completion of these trials. The experiment concluded with debriefing and payment.

** Figure 1** <br>
*Overview of the Bubbles Task* <br>
<img width="1895" height="1248" alt="image" src="https://github.com/user-attachments/assets/9774feae-5a59-47d2-bc46-fa4638c620cf" /> <br>
*Note. a) an example of the selection screen in Phase 1. b) an example of a miniature source task. c) an example of a selection screen in Phase 3 of Experiment
2. d) an example of a post-choice feedback screen. e) example bubble-clicking task. All panels show the symbols presented in Experiment 2 instead of the natural
language stimuli presented in Experiment 1. In Experiment 1 the natural language Crels and Cfuncs were “more points”, “fewer points”, “more bubbles”, and “fewer
bubbles”. In Experiment 2 the novel Crels and Cfuncs were ◗, ■, ✦, and ▾.* 

The above describes the procedure of Experiment 1 in the manuscript. The procedure of Experiment 2 was identical, except that novel stimuli for which both Crel and Cfunc functions were to be established were presented in place of natural language Crels and Cfuncs (i.e., ◗, ■, ✦, and ▾). The functional roles of these novel stimuli differed across the two counterbalancing conditions in Experiment 2. In counterbalancing
condition 1 these functional roles were: ◗ = more points, ■ = more bubbles, ✦ = less bubbles, and ▾ = less points. In counterbalancing condition 2 these functional roles were: ◗ = less bubbles, ■ = less
points, ✦ = more points, and ▾ = more bubbles. In another difference from Experiment 1 for one condition the training phase involved presenting post-choice feedback for between 3 and 10 s after each selection
screen. This feedback was not presented during testing. After completing training and testing on the bubble task participants were asked to indicate what they thought each of the Crel and Cfunc symbols presented
in Experiment 2 meant.
